{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSCI623-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kva-qoUdiVgR",
        "colab_type": "text"
      },
      "source": [
        "#**APPENDIX**\n",
        "### MSCI623- Big Data Analytics  \n",
        "###### University of Waterloo - spring 2020\n",
        "\n",
        "These codes have two goals:\n",
        "1.   Using the K-mean algorithm to clustering job titles - *Unsupervised learning*\n",
        "2.   Using a neural network to detect \"fraud\" and \"not_fraud\" job posts - *Supervised learning*  \n",
        "\n",
        "by implementing python language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "togctZV0kMI2",
        "colab_type": "text"
      },
      "source": [
        "## Preparing coding environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQxZaLNGlIcc",
        "colab_type": "text"
      },
      "source": [
        "These codes have been run in **Google Colab** and the coding environment has been set based on that.  \n",
        "  \n",
        "1.   Importing essential libraries and download some extra components of packages(need to upgrade tensorflow library)\n",
        "2.   Setting random states to have consistency in answers every time of running codes\n",
        "3.   Turning warnings off\n",
        "4.   Uploading data \n",
        "5.   Saving data as a dataframe\n",
        "6.   Setting a drive on google drive to be able to save models, pictures and graphs\n",
        "7.   Checking system GPU\n",
        "8.   Running personalized functions that will be used in the rest of the codes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnPuKym1x9xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upgrade tensorflow library\n",
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCJSIW_4uPur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import io\n",
        "import os\n",
        "import warnings\n",
        "import random\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import re \n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from wordcloud import WordCloud\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from  tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "print('All required libraries were imported')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iREdS4IiKOUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading essential components of nltk library and set them \n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words(\"english\")) \n",
        "nltk.download('punkt')\n",
        "print('\\n', '*'*15, 'All extra packages downloaded and set', '*'*15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rZaZm1j_--i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set random seeds\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "PYTHONHASHSEED=123\n",
        "print('Random seeds set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATdedoOAE-r4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turning warnings off\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print('All warnings turned off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq1abZiWpb9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload data file and wait untill become 100% done. This might takes some time\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UspQm32tt24X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving data on dataframe\n",
        "job_posts = pd.read_csv('fake_job_postings.csv')\n",
        "print('CSV file has been saved in pandas dataframe as \"job_posts\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik_fXplBLVDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set a folder to save images and tabels\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"gdrive/My Drive/MSCI 623-Project\")\n",
        "print(os.getcwd())\n",
        "print('MSCI623-Project in google drive set as repository')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wg9i7cLnyZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking the GPU of running codes (If using google colab GPU the output should be \"/device:GPU:0\")\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKcnHjNWAVZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function clean text data and return it as text\n",
        "def clean_text(text):\n",
        "  text = text.lower()  # lower case the text\n",
        "  text = re.sub(r\"[^a-z]+\", ' ', text)  # remove all signs and punctuation and numbers\n",
        "  words = word_tokenize(text)  # Tokenize data\n",
        "  wordsFiltered = []\n",
        "  for w in words:\n",
        "    if w not in stopwords:\n",
        "      wordsFiltered.append(w)\n",
        "\n",
        "  return '{}'.format(' '.join(wordsFiltered))\n",
        "\n",
        "print('Personalized function have been loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wod-fStg0Th",
        "colab_type": "text"
      },
      "source": [
        "## Exploratory data analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0BdGj8HzxlN",
        "colab_type": "text"
      },
      "source": [
        "Next codes are to explore the original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSaj2db3sw2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To check data shape\n",
        "print('# of columns:', job_posts.shape[1])\n",
        "print('# of rows:', job_posts.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Eo0nR3fu0zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To check dataframe\n",
        "job_posts.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMuEnGHUxVNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Understanding numeric columns of data \n",
        "job_posts.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X2QuA2Ks-Pw",
        "colab_type": "text"
      },
      "source": [
        "The \"job_id\" is table key and the other four numeric columns as \"telecommuting\", \"has_company_logo\", \"has_question\" and \"fraudulent\" are binary variables.  \n",
        "The \"fraudulent\" column is the target column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Z9fEPw5KDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking share of the target value\n",
        "fake_count = (job_posts[\"fraudulent\"]==1).sum()\n",
        "true_count=(job_posts[\"fraudulent\"]==0).sum()\n",
        "total = fake_count + true_count\n",
        "print('# of fake posts:',fake_count, '({:4.2f}'.format(fake_count*100/total),'% of total)')\n",
        "print('# of true posts:',true_count, '({:4.2f}'.format(true_count*100/total),'% of total')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4p8AaCK3tEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing share of the target value\n",
        "fig, axes = plt.subplots()\n",
        "plt.tight_layout()\n",
        "\n",
        "job_posts[\"fraudulent\"].value_counts().plot(kind='pie',  labels=['not_fraudulent \\n (~95%)', 'fraudulent \\n (~5%)'],colormap='Set3')\n",
        "axes.set_ylabel(' ')\n",
        "plt.title('Fraudulent and legitimate(not_fraudulent) share of data', fontsize=13)\n",
        "\n",
        "fig.savefig('Images/datashare.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpnb9bcFCFGK",
        "colab_type": "text"
      },
      "source": [
        "Data is considerably imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10m_cTdy3tQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing share of other three binary variables considering the target value\n",
        "fig, ax = plt.subplots(1,2)\n",
        "plt.tight_layout()\n",
        "\n",
        "tab1 = pd.crosstab(job_posts['fraudulent'],job_posts['telecommuting'])\n",
        "tab1.rename(columns={1: 'telecom', 0: 'not_telecom'},inplace=True)\n",
        "for i,cat in enumerate(tab1.index):\n",
        "    tab1.loc[cat].plot.pie(ax=ax[i],startangle=260, colors= np.array(['skyblue','khaki']))\n",
        "    ax[0].set_title('True_job_posts', fontweight='bold')\n",
        "    ax[1].set_title('Fake_job_posts', fontweight='bold')\n",
        "    ax[0].set_ylabel('telecommuting position')\n",
        "    ax[0].yaxis.labelpad=20.0\n",
        "    ax[1].set_ylabel('')\n",
        "fig.savefig('Images/telecom_pos_share.png')\n",
        "# plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1,2)\n",
        "plt.tight_layout()\n",
        "tab2 = pd.crosstab(job_posts['fraudulent'],job_posts['has_company_logo'])\n",
        "tab2.rename(columns={1: 'has_logo', 0: 'no_logo'},inplace=True)\n",
        "for i,cat in enumerate(tab2.index):\n",
        "    tab2.loc[cat].plot.pie(ax=ax[i],startangle=45, colors= np.array(['skyblue','khaki']))\n",
        "    ax[0].set_title('True_job_posts', fontweight='bold')\n",
        "    ax[1].set_title('Fake_job_posts', fontweight='bold')\n",
        "    ax[0].set_ylabel('has_company_logo in posts')\n",
        "    ax[0].yaxis.labelpad=20.0\n",
        "    ax[1].set_ylabel('')\n",
        "fig.savefig('Images/Logo_share.png')\n",
        "# plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1,2)\n",
        "plt.tight_layout()\n",
        "tab3 = pd.crosstab(job_posts['fraudulent'],job_posts['has_questions'])\n",
        "tab3.rename(columns={1: 'has_questin', 0: 'no_question'},inplace=True)\n",
        "for i,cat in enumerate(tab3.index):\n",
        "    tab3.loc[cat].plot.pie(ax=ax[i],startangle=180, colors= np.array(['skyblue','khaki']))\n",
        "    ax[0].set_title('True_job_posts', fontweight='bold')\n",
        "    ax[1].set_title('Fake_job_posts', fontweight='bold')\n",
        "    ax[0].set_ylabel('has_question in posts')\n",
        "    ax[0].yaxis.labelpad=20.0\n",
        "    ax[1].set_ylabel('')\n",
        "fig.savefig('Images/question_share.png')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ao9NeqvGQVC",
        "colab_type": "text"
      },
      "source": [
        "While most real job posts show the company logo, the big portion of fake job posts has no logo. Moreover, a smaller share of fake job posts has questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR7pbJCKHmzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To check the correlation between three binary variables and target value\n",
        "cor_data = job_posts[['fraudulent','telecommuting','has_company_logo','has_questions']]\n",
        "corr = cor_data .corr(method=\"pearson\")\n",
        "\n",
        "fig, axis= plt.subplots(figsize=(8, 8))\n",
        "plt.tight_layout()\n",
        "\n",
        "sns.heatmap(corr, cmap= \"RdBu\", center=0.00, annot=True, fmt='.1g',cbar_kws={'label': 'correlation'})\n",
        "sns.set(font_scale=1.3)\n",
        "\n",
        "fig.savefig('Images/binary_cor.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krkZtQ1xMsMX",
        "colab_type": "text"
      },
      "source": [
        "Though there is no high correlation between variables, telecommuting position shows a bigger positive correlation with target value in comparison to the other two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUN2iAepEkLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking missing values in the data set\n",
        "NA = job_posts.isna().groupby(job_posts.fraudulent).sum().T\n",
        "NA['Total']= NA.sum(axis=1)\n",
        "NA.to_csv('data/NA_table.csv', index=True, encoding='utf-8')\n",
        "NA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziSaEhJQ23VS",
        "colab_type": "text"
      },
      "source": [
        "The data set has a lot of missing information. However, based on the text mining approach in this project, all text columns will be considered as a package of text information. Thus, the only concern will be the salary range column, which could be eliminated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqH4QrA28kE",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T60Y7wWHGRrY",
        "colab_type": "text"
      },
      "source": [
        "All 12 columns as \"title\", \"location\", \"department\", \"company_profile\", \"description\", \"requirements\", \"benefits\", \"employement_type\", \"required_experience\", \"required_education\", \"industry\" and \"function\" will be combined in information column and the original columns will be removed, except for \"title\" column. The \"title\" column is needed for unsupervised learnin(clustering)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndLsXAyaE8lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To keep the main dataset and make some changes to the new one\n",
        "data = job_posts.copy()\n",
        "data.fillna(\" \",inplace = True)\n",
        "\n",
        "data.drop(['job_id'], axis=1, inplace=True)\n",
        "data['information'] = data['title']  + ' ' + data['location'] + ' '  + ' ' + data['department'] + data['company_profile'] + ' ' + data['description'] + ' ' + data['requirements'] + ' ' + data['benefits'] + ' ' + data['employment_type'] + ' ' + data['required_experience'] + ' ' + data['required_education'] + ' ' + data['industry'] + ' ' + data['function'] \n",
        "\n",
        "data.drop(['location', 'department', 'company_profile','description', 'requirements', 'benefits', 'employment_type','required_experience','required_education', 'industry', 'function'], axis=1, inplace=True)\n",
        "data.drop(['telecommuting','has_company_logo','has_questions'],axis=1,inplace=True)\n",
        "data.drop(['salary_range'], axis=1, inplace=True)\n",
        "data=data[['title','information','fraudulent']]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Jo7sBGRJ3C",
        "colab_type": "text"
      },
      "source": [
        "The new data set has no missing value in the remaining columns and data is ready for the next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3noNXTzVbtYm",
        "colab_type": "text"
      },
      "source": [
        "## Unsupervised learnin - Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjFDZfsLbm3N",
        "colab_type": "text"
      },
      "source": [
        "The **k-means clustering** algorithm will be used to clustering **job titles** for unsupervised learning.   \n",
        "The k_means algorithm expect numeric data as input. Thus, the **doc2vec** approach will be applied to convert alphabetic \"title\" data to a numeric format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOWxaSiZ1hXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To cleaning text data and check it(this might takes some time)\n",
        "df=data.copy()\n",
        "df.information=df.information.apply(lambda x: clean_text(x)) \n",
        "df.title=df.title.apply(lambda x: clean_text(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDDOWmrKhd14",
        "colab_type": "text"
      },
      "source": [
        "###Doc2vec for job titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAurYAFhGivl",
        "colab_type": "text"
      },
      "source": [
        "Document to vector(doc2vec) model of \"gensim\" library will be implemented to convert titles to vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDsi1Ugypk2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title_len = list(df.title.apply(lambda x : len(x)))\n",
        "print('\"Title length information\"')\n",
        "print(' Minimun:', int(np.min(title_len)) )\n",
        "print(' Mode:', int(statistics.mode(title_len)) )\n",
        "print(' 99 percentile:', int(np.percentile(title_len, 99)) )\n",
        "print(' Maximum:', int(np.max(title_len)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL_ATpq8rbAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To prepare data for training doc2vec model for titles\n",
        "titles = list(df.title.apply(lambda x: word_tokenize(x)))\n",
        "tagged_titles= [TaggedDocument(doc, [i]) for i, doc in enumerate(titles)]\n",
        "print('Title data are ready to be used to train doc2vec model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmLTon_hrbLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create doc2vec model for titles (this might takes some time)\n",
        "d2v_titels = Doc2Vec(vector_size=20, min_count=2, epochs=10)\n",
        "d2v_titels.build_vocab(tagged_titles)\n",
        "d2v_titels.train(tagged_titles, total_examples=d2v_titels.corpus_count, epochs=d2v_titels.epochs)\n",
        "print('Doc2Vec model for titles trained')\n",
        "\n",
        "d2v_titels.save('data/d2v_titels.model')\n",
        "print('Doc2vec model for titles saved in the data folder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwBHRUSmyQZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting the doc2vec vectors for the job titles\n",
        "titles_vec = d2v_titels.docvecs.vectors_docs\n",
        "print('All',len(titles_vec),'job titles converted to their vectors.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajXj9q2ULgJv",
        "colab_type": "text"
      },
      "source": [
        "###k_mean clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8KVro3CZLvP",
        "colab_type": "text"
      },
      "source": [
        "To find the optimal number of clusters, the visual approach of \"Elbow method\" will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjufzsPGTLqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing different amount of k for k-mean model and calculate SSE for them (this might takes some time)\n",
        "distortations = {}\n",
        "for k in range(1,25):\n",
        "  kmeans = KMeans(n_clusters=k,init='k-means++') \n",
        "  kmeans.fit(titles_vec)\n",
        "  distortations[k] = kmeans.inertia_\n",
        "print('Data for different amount of K has been saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if3ECbCzZoQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting elbow curve \n",
        "fig, axes = plt.subplots(figsize=(8, 8))\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.plot(list(distortations.keys()),list(distortations.values()), color='red')\n",
        "plt.title('Elbow curve to find number of clusters for titles')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('SSE')\n",
        "axes.set_facecolor('white')\n",
        "plt.grid(b=True, which='major', color='lightgrey', linestyle='--', axis='x')\n",
        "plt.grid(b=None, which='minor', color='lightgrey', linestyle=':', axis='x')\n",
        "plt.minorticks_on()\n",
        "axes.spines['bottom'].set_color('0.5')\n",
        "axes.spines['top'].set_color('0.5')\n",
        "axes.spines['right'].set_color('0.5')\n",
        "axes.spines['left'].set_color('0.5')\n",
        "\n",
        "fig.savefig('Images/elbow.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjXpaTfMahYe",
        "colab_type": "text"
      },
      "source": [
        "Thus, the best number of clusters could be 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsWlF2rDyQVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To model K-mean\n",
        "km_model = KMeans(n_clusters=6, init='k-means++')  \n",
        "km_model.fit(titles_vec)\n",
        "labels=km_model.labels_.tolist()\n",
        "print('k-mean clustering model has been created with k=6')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efdS8CLF-NqH",
        "colab_type": "text"
      },
      "source": [
        "The \"Principal Component Analysis(PCA)\" will be used to plot the clusters and show how the model performs visually. PCA makes it easier to see clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "422vXqFTyQP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To visualize the clustering model\n",
        "predict = km_model.fit_predict(titles_vec)\n",
        "pca = PCA(n_components=2).fit(titles_vec)\n",
        "datapoint = pca.transform(titles_vec)\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(8, 8))\n",
        "plt.tight_layout()\n",
        "\n",
        "color_theme = ['darkgray','lightsalmon','powderblue','steelblue','gold', 'darkkhaki','turquoise','hotpink']\n",
        "color = [color_theme[i] for i in predict]\n",
        "\n",
        "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
        "centroids = km_model.cluster_centers_\n",
        "centroidpoint = pca.transform(centroids)\n",
        "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='black')\n",
        "plt.title('K-Means Classification')\n",
        "axes.set_facecolor('white')\n",
        "axes.grid()\n",
        "axes.spines['bottom'].set_color('0.5')\n",
        "axes.spines['top'].set_color('0.5')\n",
        "axes.spines['right'].set_color('0.5')\n",
        "axes.spines['left'].set_color('0.5')\n",
        "\n",
        "fig.savefig('Images/clusters.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-nNf6tQVbcz",
        "colab_type": "text"
      },
      "source": [
        "Now we will add labels to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4ZLBqvSVb1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # To check the dataset\n",
        "df['title_cluster']= labels\n",
        "df=df[['title','title_cluster','information','fraudulent']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ6dCwjYilsD",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the number of data in each clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7LLlK3f_cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To count number of titles in each cluster\n",
        "c_table = df.groupby('title_cluster').count()\n",
        "c_table.drop(['information','fraudulent'],axis=1,inplace=True)\n",
        "c_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auCr5ZoEinBQ",
        "colab_type": "text"
      },
      "source": [
        "## Supervised learning - Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz3VPm7Ui2Pb",
        "colab_type": "text"
      },
      "source": [
        "The goal is to classify job posts into fraudulent and legitimate based on the text information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XgUd2y-kE9B",
        "colab_type": "text"
      },
      "source": [
        "### Data preperation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVcNbzGY4UrO",
        "colab_type": "text"
      },
      "source": [
        "To apply text mining approaches to the information column, it is good to get a better insight into the texts in that column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v77x3almmrlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To preparing data for classification\n",
        "cdata = df.copy()\n",
        "cdata['tokenized_info']= cdata['information'].apply(lambda x: word_tokenize(x))\n",
        "cdata.drop(['title','title_cluster'],axis=1,inplace=True)\n",
        "cdata = cdata[['information','tokenized_info','fraudulent']]\n",
        "cdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdFPo5V09Z4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting an idea about the information column\n",
        "fake_info_len = list(cdata[cdata[\"fraudulent\"]==1]['tokenized_info'].apply(lambda x: len(x)))\n",
        "true_info_len = list(cdata[cdata[\"fraudulent\"]==0]['tokenized_info'].apply(lambda x: len(x)))\n",
        "all_info_len = [*true_info_len, *fake_info_len]\n",
        "print('number of fake sentences:',len(fake_info_len))\n",
        "print('number of true sentences:',len(true_info_len))\n",
        "print('Maximum number of words in information part for fake posts:',max(fake_info_len))\n",
        "print('Maximum number of words in information part for true posts:',max(true_info_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA1M8ZSxkIiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualizing the length of data in the information column\n",
        "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(15, 5))\n",
        "\n",
        "ax1.hist(fake_info_len,bins = 20,color='lightsalmon')\n",
        "ax1.set_title('fraudulent posts')\n",
        "ax1.set_xlabel('Number of words', fontweight='bold',fontsize=11)\n",
        "ax1.set_facecolor('white')\n",
        "ax1.spines['bottom'].set_color('0.5')\n",
        "ax1.spines['top'].set_color('0.5')\n",
        "ax1.spines['right'].set_color('0.5')\n",
        "ax1.spines['left'].set_color('0.5')\n",
        "\n",
        "ax2.hist(true_info_len, bins = 20,color='steelblue')\n",
        "ax2.set_title('not_fraudulent Post')\n",
        "ax2.set_xlabel('Number of words', fontweight='bold',fontsize=11)\n",
        "ax2.set_facecolor('white')\n",
        "ax2.spines['bottom'].set_color('0.5')\n",
        "ax2.spines['top'].set_color('0.5')\n",
        "ax2.spines['right'].set_color('0.5')\n",
        "ax2.spines['left'].set_color('0.5')\n",
        "\n",
        "fig.suptitle('Number of words in information part',fontweight='bold',fontsize=12)\n",
        "\n",
        "fig.savefig('Images/info_len.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5SQmFgeBi4O",
        "colab_type": "text"
      },
      "source": [
        "The \"word cloud\" is a famous visual method in natural language processing to get an insight into text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlMZEW_yBjb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using visualization to see the most frequent tokens\n",
        "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(15, 5), dpi=100)\n",
        "plt.tight_layout()\n",
        "\n",
        "wc_fake = WordCloud(width = 1400 , height = 800 , max_words = 500 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(\" \".join(cdata[cdata.fraudulent == 1].information))\n",
        "ax1.imshow(wc_fake)\n",
        "ax1.set_title('fraudulent posts')\n",
        "ax1.axis('off')\n",
        "\n",
        "wc_true = WordCloud(width = 1400 , height = 800 , max_words = 500 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(\" \".join(cdata[cdata.fraudulent == 0].information))\n",
        "ax2.imshow(wc_true)\n",
        "ax2.set_title('not_fraudulent posts')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.axis(\"off\") \n",
        "plt.grid(b=None)\n",
        "\n",
        "fig.savefig('Images/wordclouds.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTV4XoKQt8rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To check data types\n",
        "print('Dataset data types')\n",
        "print(cdata.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ0pop17zygs",
        "colab_type": "text"
      },
      "source": [
        "For unsupervised learning, data has been split into three parts. \"Training\" data will be used to train the model, \"Validation\" will be applied for tuning the model's hyperparameters and \"Test set\" will be used to check the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKFzpVunkIW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting data into three sets\n",
        "x= list(cdata.tokenized_info)\n",
        "y= to_categorical(np.array(cdata.fraudulent))\n",
        "\n",
        "train_x , val_x ,train_y , val_y = train_test_split( x , y , test_size = 0.1 , random_state = 123)\n",
        "train_x , test_x ,train_y , test_y = train_test_split( x , y , test_size = 0.1 , random_state = 123)\n",
        "\n",
        "print('Data randomly splited to the training set(80%), validation set(10%) and test set(10%)')\n",
        "print('*'*20)\n",
        "print('training set size=', len(train_x))\n",
        "print('validation set size=', len(val_x))\n",
        "print('testing set size=', len(test_x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLE3Zfjbhjwc",
        "colab_type": "text"
      },
      "source": [
        "### Doc2vec for job posts text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAy8iC04rfEW",
        "colab_type": "text"
      },
      "source": [
        "Document to vector model will be applied to the information column which contains all text information about the job posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njQbNbD4hitI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare training data to be used  in doc to vec model\n",
        "tagged_info= [TaggedDocument(doc, [i]) for i, doc in enumerate(train_x)]\n",
        "print('Training data is ready to be used for doc2vec model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBHILVbVhjNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building Doc2vec model for information column\n",
        "d2v_info = Doc2Vec(vector_size=100, min_count=1, epochs=20)\n",
        "d2v_info.build_vocab(tagged_info)\n",
        "d2v_info.train(tagged_info, total_examples=d2v_info.corpus_count, epochs=d2v_info.epochs)\n",
        "print('Doc2Vec model has been trained for information column')\n",
        "\n",
        "d2v_info.save('data/d2v_info.model')\n",
        "print('Doc2vec model for information column saved in data folder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC_u4UjaATrT",
        "colab_type": "text"
      },
      "source": [
        "### Pre Processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxLHU5NutV3O",
        "colab_type": "text"
      },
      "source": [
        "All three data sets information have to be converted into vectors to be used as an input of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVrr19aakt3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre_processing data (this might takes some time)\n",
        "train_vec= np.array([d2v_info.infer_vector(item) for item in train_x])\n",
        "val_vec =  np.array([d2v_info.infer_vector(item) for item in val_x])\n",
        "test_vec = np.array([d2v_info.infer_vector(item) for item in test_x])\n",
        "print('pre-processing data is finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-7iKKwT8X0l",
        "colab_type": "text"
      },
      "source": [
        "### Classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfDoB8vEmWRD",
        "colab_type": "text"
      },
      "source": [
        "Two classifier will be used for classificatioin, **Decision tree** and **Neural networks**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILcHY5z05W1k",
        "colab_type": "text"
      },
      "source": [
        "#### Decision tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITwy8ufj-Dts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To create and fit the decision tree model\n",
        "dt_model = DecisionTreeClassifier(class_weight='balanced', criterion = 'entropy')\n",
        "dt_model.fit(train_vec,np.argmax(train_y,axis=1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meJRk_Z86CqF",
        "colab_type": "text"
      },
      "source": [
        "##### Testing decision tree model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PacyogBd_p-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To apply model on test set\n",
        "dt_pred= dt_model.predict(test_vec)\n",
        "dt_report = classification_report(np.argmax(test_y,axis=1),dt_pred,target_names = ['0','1'])\n",
        "print(dt_report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijpM5_np502L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To graph the confusion matrix\n",
        "dt_cm=confusion_matrix(np.argmax(test_y,axis=1),dt_pred)\n",
        "dt_cm = pd.DataFrame(dt_cm)\n",
        "dt_cm.index.name = 'Actual'\n",
        "dt_cm.columns.name = 'Predicted'\n",
        "\n",
        "fig, axis= plt.subplots(figsize=(8, 8))\n",
        "plt.tight_layout()\n",
        "\n",
        "sns.heatmap(dt_cm ,cmap= \"Blues\",annot = True, fmt='')\n",
        "\n",
        "fig.savefig('Images/dt_confusion_matrix.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQhkqWpx5lmW",
        "colab_type": "text"
      },
      "source": [
        "#### Neural network - Sequential mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrvI5A_a_ksl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining model \n",
        "model = Sequential(name='Neural_Model')\n",
        "model.add(Dense(512, activation='relu', input_dim=100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ECmwNCYdxf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To plot the model\n",
        "plot_model(model, to_file='Images/Neural_model.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-FH6h5qPudk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting model (this takes some times)\n",
        "weights = {0:5, 1:100}\n",
        "history = model.fit(train_vec, train_y, class_weight=weights, batch_size=64, epochs=100, validation_data=(val_vec, val_y))\n",
        "print('\\n','*'*20,'Model trained','*'*20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8cgdDTpuY2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving sequentioal model\n",
        "model.save('data/sequential_model')\n",
        "print('Sequential model has been saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Dxy8wzBJqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting the accuracy and loss of the training and validation sets during epochs\n",
        "fig, (ax1,ax2)= plt.subplots(ncols=2, figsize=(10, 5), dpi=100)\n",
        "\n",
        "epochs = [i for i in range(100)]\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "fig.set_size_inches(20,10)\n",
        "\n",
        "ax1.plot(epochs , train_acc , 'co-' , label = 'Training Accuracy')\n",
        "ax1.plot(epochs , val_acc , 'yo-' , label = 'Validation Accuracy')\n",
        "ax1.set_title('Training & Validation Accuracy per epochs',fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_facecolor('white')\n",
        "ax1.spines['bottom'].set_color('0.5')\n",
        "ax1.spines['top'].set_color('0.5')\n",
        "ax1.spines['right'].set_color('0.5')\n",
        "ax1.spines['left'].set_color('0.5')\n",
        "ax2.plot(epochs , train_loss , 'co-' , label = 'Training Loss')\n",
        "ax2.plot(epochs , val_loss , 'yo-' , label = 'Validation Loss')\n",
        "ax2.set_title('Training & Validation Loss per epochs', fontsize=12, fontweight='bold')\n",
        "ax2.legend( fontsize= 13)\n",
        "ax2.set_xlabel(\"Epochs\")\n",
        "ax2.set_ylabel(\"Loss\")\n",
        "ax2.set_facecolor('white')\n",
        "ax2.spines['bottom'].set_color('0.5')\n",
        "ax2.spines['top'].set_color('0.5')\n",
        "ax2.spines['right'].set_color('0.5')\n",
        "ax2.spines['left'].set_color('0.5')\n",
        "fig.suptitle('Fitting model history per epoches',fontweight='bold')\n",
        "\n",
        "fig.savefig('Images/learning_loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttf-er3NBAij",
        "colab_type": "text"
      },
      "source": [
        "##### Testing Neural model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJAXrUq9PurB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To apply model on test set\n",
        "pred_prop =model.predict(test_vec)\n",
        "pred=np.around(pred_prop , decimals = 0)\n",
        "report = classification_report(test_y,pred)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gimwIzmPu1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To graph the confusion matrix\n",
        "cm=confusion_matrix(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))\n",
        "cm = pd.DataFrame(cm)\n",
        "cm.index.name = 'Actual'\n",
        "cm.columns.name = 'Predicted'\n",
        "\n",
        "fig, axis= plt.subplots(figsize=(8, 8))\n",
        "plt.tight_layout()\n",
        "\n",
        "sns.heatmap(cm ,cmap= \"Blues\",annot = True, fmt='')\n",
        "\n",
        "fig.savefig('Images/confusion_matrix.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}