# -*- coding: utf-8 -*-
"""MSCI641_8Aug.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k1-MvM-lDQOyQutOaO9o_mMJoSyviUaX

# **Appendix 2**

## **MSCI 641 - Text analytics**
## Final courese project  
#### University of Waterloo - spring 2020.

## Preparing coding environment

These codes have been run in **Google Colab** and the coding environment has been set based on that.  
  
1.   Importing essential libraries and download some extra components of packages(need to upgrade tensorflow library)
2.   Setting random states to have consistency in answers every time of running codes
3.   Turning warnings off
4.   Uploading data 
5.   Saving data as a dataframe
6.   Setting a drive on google drive to be able to save models, pictures and graphs
7.   Checking system GPU
8.   Running personalized functions that will be used in the rest of the codes
"""

# To upgrade tensorflow library
!pip install tensorflow
!pip install --upgrade tensorflow
!pip install tf-nightly
!pip install keras

# To import requred libraries
import io
import os
import warnings
import random
import statistics
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
import re 
import nltk
import keras
import gensim
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from wordcloud import WordCloud
from textblob import TextBlob
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from sklearn.linear_model import LogisticRegression
from sklearn import metrics 
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.utils import plot_model

print('All required libraries were imported')

# To download essential components of nltk library and set them
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
stopwords = set(nltk.corpus.stopwords.words("english")) 
lemmatizer = WordNetLemmatizer()
print('\n', '*'*15, 'All extra packages downloaded and set', '*'*15)

# To set random seeds
random.seed(123)
np.random.seed(123)
tf.random.set_seed(123)
PYTHONHASHSEED=123
print('Random seeds set')

# To turn warnings off
warnings.filterwarnings("ignore")
print('All warnings turned off')

# To upload data file and wait untill become 100% done. This might takes some time
'''Select all 4 .csv files which are needed in text codes as: 
"train_bodies.csv", "train_stances.csv","test_bodies.csv", "test_stances_unlabeled.csv"'''
from google.colab import files
uploaded = files.upload()

# To save data on dataframe
train_bodies = pd.read_csv(io.BytesIO(uploaded['train_bodies.csv']))
train_stances = pd.read_csv(io.BytesIO(uploaded['train_stances.csv']))
test_bodies = pd.read_csv(io.BytesIO(uploaded['test_bodies.csv']))
test_stances_unlabeled = pd.read_csv(io.BytesIO(uploaded['test_stances_unlabeled.csv']))
print('All 4 required datasets are stored in dataframes with same name as their .csv files')

# To set a folder to save images and tabels
from google.colab import drive
drive.mount('/content/gdrive')
os.chdir("gdrive/My Drive/MSCI 641-Project")
print(os.getcwd())
print('MSCI641-Project in google drive set as repository')

# To check the GPU of running codes (If using google colab GPU the output should be "/device:GPU:0")
tf.test.gpu_device_name()

"""These functions will be used later on this project."""

# To define some personalized functions

# This will combine two seperate .csv files and return a dataframe
def create_data_set ( headline_stance_file,  body_file):
  dataset = pd.merge(headline_stance_file, body_file ,on ='Body ID',how ='outer')
  dataset.drop(['Body ID'],axis=1,inplace=True)
  return dataset

# To make words Pos tags similar
def get_simple_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN


# This function clean text data and return it as text
def clean_text(text):
  text = text.lower()  # lower case the text
  text = re.sub(r"[^a-z]+", ' ', text)  # remove all signs and punctuation and numbers
  words = word_tokenize(text)  # Tokenize data
  wordsFiltered = []
  lemmalist= []
  for w in words:
    if w not in stopwords:
      wordsFiltered.append(w)
      pos = pos_tag([w])
      lemma = lemmatizer.lemmatize(w,get_simple_pos(pos[0][1]))
      lemmalist.append(lemma)
  return lemmalist
#  return '{}'.format(' '.join(lemmalist))


# This function remove characters and lower case and return data as text
def no_char(text):
  text = text.lower()  # lower case the text
  text = re.sub(r"[^a-z0-9_]+", ' ', text)  # remove all signs and punctuation and numbers
  return text


# This function returns the score of refutation words in text
def refutation_score(clean_text):
  score=0
  for words in clean_text:
    for w in refuting_words:
      score = score + 1
  return score


# This function gives each pair of headline and body and return the amount of overlapping between them
def overlapping(list_cleaned_headline , list_cleaned_body):
  overlap = len(set(list_cleaned_headline).intersection(list_cleaned_body)) / float(len(set(list_cleaned_headline).union(list_cleaned_body)))
  return [overlap]


print('Personalized functions have been loaded')

"""##Exploratory data analysis (EDA)

Next codes are to explore the original data.
"""

# To join training dataframe
train_df = create_data_set (train_stances, train_bodies)
train_df = train_df[['Headline','articleBody','Stance']]
train_df.head()

# To join test dataframe
test_df = create_data_set (test_stances_unlabeled, test_bodies)
test_df.head()

# To check data shape
print('Training dataset:')
print('# of columns:', train_df.shape[1])
print('# of rows:', train_df.shape[0])
print('\n', '*'*15, '\n')
print('Test dataset:')
print('# of columns:', test_df.shape[1])
print('# of rows:', test_df.shape[0])

# To check data types
print('Training dataset data types')
print(train_df.dtypes)
print('\n', '*'*15, '\n')
print('Test dataset data types')
print(test_df.dtypes)

# To check missing values
print('training dataset missing values')
print(train_df.isna().sum().T)
print('\n', '*'*15, '\n')
print('test dataset missing values')
print(test_df.isna().sum().T)

"""From now, only the training dataframe will be explored and test dataframe will be used only to evaluate the performance of the model."""

# To calculate the share of data in training set
unrelated_count = (train_df['Stance']=='unrelated').sum()
agree_count = (train_df['Stance']=='agree').sum()
disagree_count = (train_df['Stance']=='disagree').sum()
discuss_count = (train_df['Stance']=='discuss').sum()
total=train_df.shape[0]
total_related = agree_count + disagree_count + discuss_count
print('# of rows:', total)
print('# of unrelated posts:',unrelated_count, '({:4.2f}'.format(unrelated_count*100/total),'% of total)')
print('# of agree posts:',agree_count, '({:4.2f}'.format(agree_count*100/total),'% of total & {:4.2f}'.format(agree_count*100/total_related),'of related data)')
print('# of disagree posts:',disagree_count, '({:4.2f}'.format(disagree_count*100/total),'% of total & {:4.2f}'.format(disagree_count*100/total_related),'of related data)')
print('# of discuss posts:',discuss_count, '({:4.2f}'.format(discuss_count*100/total),'% of total & {:4.2f}'.format(discuss_count*100/total_related),'of related data)')

# To check dataframe
train_df.head()

# To visually show the share of data
fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(10, 5), dpi=100)
plt.tight_layout()

train_df['category'].value_counts().plot(kind='pie',colormap='Set3', ax=ax1 ,startangle=360, labels=['unrelated \n (~73%)','discuss \n (~18%)','agree \n (~7%)', 'disagree \n (~2%)'])
ax1.set_ylabel(' ')
ax1.set_title('Share of all four labels \n  in training data set', fontsize=11)

train_df[train_df['category'] != 3 ]['category'].value_counts().plot(kind='pie',colormap='Set3', ax=ax2 , startangle=120, labels=['discuss \n (~66%)','agree \n (~28%)', 'disagree \n (~6%)'])
ax2.set_ylabel(' ')
ax2.set_title('Share of other three labels \n in "related"group of training data set', fontsize=11)


fig.savefig('Images/datashare.png')
plt.show()

# To use wordcloud visualization to see the most frequent tokens in headlines 
fig,ax= plt.subplots(nrows=2, ncols=2 , figsize=(13, 9), dpi=100)
plt.tight_layout()

wc_unelated = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 3].Headline))
ax[0][0].imshow(wc_unelated)
ax[0][0].set_title('Unrelated Stances')
ax[0][0].axis('off')

wc_discuss = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 2].Headline))
ax[0][1].imshow(wc_discuss)
ax[0][1].set_title('Discuss Stances')
ax[0][1].axis('off')

wc_agree = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 0].Headline))
ax[1][0].imshow(wc_agree)
ax[1][0].set_title('Agree Stances')
ax[1][0].axis('off')

wc_disagree = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 1].Headline))
ax[1][1].imshow(wc_disagree)
ax[1][1].set_title('Disagree Stances')
ax[1][1].axis('off')

plt.axis("off") 
plt.grid(b=None)
fig.suptitle('Most frequent words in "Headlines"',fontweight='bold',fontsize=15)
plt.subplots_adjust(wspace=0.05, hspace=0, top=0.95)

fig.savefig('Images/Hedlines_wordclouds.png')
plt.show()

# To use wordcloud visualization to see the most frequent tokens in article body 
fig,ax= plt.subplots(nrows=2, ncols=2 , figsize=(13, 9), dpi=100)
plt.tight_layout()

wc_unelated = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 3].articleBody))
ax[0][0].imshow(wc_unelated)
ax[0][0].set_title('Unrelated Stances')
ax[0][0].axis('off')

wc_discuss = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 2].articleBody))
ax[0][1].imshow(wc_discuss)
ax[0][1].set_title('Discuss Stances')
ax[0][1].axis('off')

wc_agree = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 0].articleBody))
ax[1][0].imshow(wc_agree)
ax[1][0].set_title('Agree Stances')
ax[1][0].axis('off')

wc_disagree = WordCloud(width = 500 , height = 300 , max_words = 1000 , background_color ='white', stopwords = stopwords, min_font_size = 8).generate(" ".join(train_df[train_df.category == 1].articleBody))
ax[1][1].imshow(wc_disagree)
ax[1][1].set_title('Disagree Stances')
ax[1][1].axis('off')

plt.axis("off") 
plt.grid(b=None)
fig.suptitle('Most frequent words in "Article_Body"',fontweight='bold',fontsize=15)
plt.subplots_adjust(wspace=0.05, hspace=0, top=0.95)

fig.savefig('Images/body_wordclouds.png')
plt.show()

# To use visualization to compare the length of texts in headlines 
fig,ax= plt.subplots(nrows=2, ncols=2 , figsize=(13, 9), dpi=100)
plt.tight_layout()

ax[0][0].hist(train_df[train_df.category == 3].Headline.str.len(),bins = 20,color='lightsteelblue')
ax[0][0].set_title('Unrelated Stances', fontweight='bold')
ax[0][0].set_xlabel('Number of words',fontsize=11)
ax[0][0].set_facecolor('white')

ax[0][1].hist(train_df[train_df.category == 2].Headline.str.len(),bins = 20,color='lightsteelblue')
ax[0][1].set_title('Discuss Stances', fontweight='bold')
ax[0][1].set_xlabel('Number of words',fontsize=11)
ax[0][1].set_facecolor('white')

ax[1][0].hist(train_df[train_df.category == 0].Headline.str.len(),bins = 20,color='lightsteelblue')
ax[1][0].set_title('Agree Stances', fontweight='bold')
ax[1][0].set_xlabel('Number of words',fontsize=11)
ax[1][0].set_facecolor('white')

ax[1][1].hist(train_df[train_df.category == 1].Headline.str.len(),bins = 20,color='lightsteelblue')
ax[1][1].set_title('Disagree Stances',fontweight='bold')
ax[1][1].set_xlabel('Number of words', fontsize=11)
ax[1][1].set_facecolor('white')

fig.suptitle('Length of "Headlines"',fontweight='bold',fontsize=15)
plt.subplots_adjust(wspace=0.1, hspace=0.3, top=0.9)

fig.savefig('Images/headlines_length.png')
plt.show()

# To use visualization to compare the length of texts in article bodies 
fig,ax= plt.subplots(nrows=2, ncols=2 , figsize=(13, 9), dpi=100)
plt.tight_layout()

ax[0][0].hist(train_df[train_df.category == 3].articleBody.str.len(),bins = 20,color='steelblue')
ax[0][0].set_title('Unrelated Stances', fontweight='bold')
ax[0][0].set_xlabel('Number of words',fontsize=11)
ax[0][0].set_facecolor('white')

ax[0][1].hist(train_df[train_df.category == 2].articleBody.str.len(),bins = 20,color='steelblue')
ax[0][1].set_title('Discuss Stances', fontweight='bold')
ax[0][1].set_xlabel('Number of words',fontsize=11)
ax[0][1].set_facecolor('white')

ax[1][0].hist(train_df[train_df.category == 0].articleBody.str.len(),bins = 20,color='steelblue')
ax[1][0].set_title('Agree Stances', fontweight='bold')
ax[1][0].set_xlabel('Number of words',fontsize=11)
ax[1][0].set_facecolor('white')

ax[1][1].hist(train_df[train_df.category == 1].articleBody.str.len(),bins = 20,color='steelblue')
ax[1][1].set_title('Disagree Stances',fontweight='bold')
ax[1][1].set_xlabel('Number of words', fontsize=11)
ax[1][1].set_facecolor('white')

fig.suptitle('Length of "Articte_body"',fontweight='bold',fontsize=15)
plt.subplots_adjust(wspace=0.1, hspace=0.3, top=0.9)

fig.savefig('Images/body_length.png')
plt.show()

# To use visualization on number of unique words in headlines 
fig,ax= plt.subplots(nrows=2, ncols=2 , figsize=(13, 9), dpi=100)
plt.tight_layout()

ax[0][0].hist(train_df[train_df.category == 3].Headline.str.split().map(lambda x: len(x)),bins = 20,color='sandybrown')
ax[0][0].set_title('Unrelated Stances', fontweight='bold')
ax[0][0].set_xlabel('Number of words',fontsize=11)
ax[0][0].set_facecolor('white')

ax[0][1].hist(train_df[train_df.category == 2].Headline.str.split().map(lambda x: len(x)),bins = 20,color='sandybrown')
ax[0][1].set_title('Discuss Stances', fontweight='bold')
ax[0][1].set_xlabel('Number of words',fontsize=11)
ax[0][1].set_facecolor('white')

ax[1][0].hist(train_df[train_df.category == 0].Headline.str.split().map(lambda x: len(x)),bins = 20,color='sandybrown')
ax[1][0].set_title('Agree Stances', fontweight='bold')
ax[1][0].set_xlabel('Number of words',fontsize=11)
ax[1][0].set_facecolor('white')

ax[1][1].hist(train_df[train_df.category == 1].Headline.str.split().map(lambda x: len(x)),bins = 20,color='sandybrown')
ax[1][1].set_title('Disagree Stances',fontweight='bold')
ax[1][1].set_xlabel('Number of words', fontsize=11)
ax[1][1].set_facecolor('white')

fig.suptitle('Number of words in "Headlines"',fontweight='bold',fontsize=15)
plt.subplots_adjust(wspace=0.1, hspace=0.3, top=0.9)

fig.savefig('Images/headlines_no_words.png')
plt.show()

# To use visualization on number of unoque words in article bodies 
fig,ax= plt.subplots(nrows=2, ncols=2 , figsize=(13, 9), dpi=100)
plt.tight_layout()

ax[0][0].hist(train_df[train_df.category == 3].articleBody.str.split().map(lambda x: len(x)),bins = 20,color='peru')
ax[0][0].set_title('Unrelated Stances', fontweight='bold')
ax[0][0].set_xlabel('Number of words',fontsize=11)
ax[0][0].set_facecolor('white')

ax[0][1].hist(train_df[train_df.category == 2].articleBody.str.split().map(lambda x: len(x)),bins = 20,color='peru')
ax[0][1].set_title('Discuss Stances', fontweight='bold')
ax[0][1].set_xlabel('Number of words',fontsize=11)
ax[0][1].set_facecolor('white')

ax[1][0].hist(train_df[train_df.category == 0].articleBody.str.split().map(lambda x: len(x)),bins = 20,color='peru')
ax[1][0].set_title('Agree Stances', fontweight='bold')
ax[1][0].set_xlabel('Number of words',fontsize=11)
ax[1][0].set_facecolor('white')

ax[1][1].hist(train_df[train_df.category == 1].articleBody.str.split().map(lambda x: len(x)),bins = 20,color='peru')
ax[1][1].set_title('Disagree Stances',fontweight='bold')
ax[1][1].set_xlabel('Number of words', fontsize=11)
ax[1][1].set_facecolor('white')

fig.suptitle('Number of words in "Articte_body"',fontweight='bold',fontsize=15)
plt.subplots_adjust(wspace=0.1, hspace=0.3, top=0.9)

fig.savefig('Images/body_no_words.png')
plt.show()

"""## Data preperation

In this section, data will be cleaned and tokenized for further modelling.
"""

# To keep the first data unchanged
df = train_df.copy()
print('Dataframe has been copied')

# To create a new numeric column for the target variable(stance)
df['Stance'] = pd.Categorical(df.Stance)
df['category'] = df.Stance.cat.codes
cat_dic = dict(zip(df.Stance.cat.codes, df.Stance))
print('Data stance have been change to categorical data based on dictionary as:\n',cat_dic)

# To remove punctuations, stop words, lowercase texts and lemmatize it and create four new columns(this might takes some time)
df['tokenized_headline'] = df['Headline'].apply(lambda x: clean_text(x))
df['tokenized_body'] = df['articleBody'].apply(lambda x: clean_text(x))
df['clean_headline'] = df['Headline'].apply(lambda x: no_char(x))
df['clean_body'] = df['articleBody'].apply(lambda x: no_char(x))
print('New columns with clean and tokenized data have been created')

# To check the dataset
df.drop(['Headline','articleBody', 'Stance'], axis=1, inplace=True)
df = df[['clean_headline','clean_body','tokenized_headline','tokenized_body','category']]
df.head()

# To Count the length of tokenized_headline and tokenized_body
title_len = list(df.tokenized_headline.apply(lambda x : len(x)))
body_len = list(df.tokenized_body.apply(lambda x : len(x)))
print('"Clean headline length"')
print(' Minimun:', int(np.min(title_len)) )
print(' Mode:', int(statistics.mode(title_len)) )
print(' 99 percentile:', int(np.percentile(title_len, 99)) )
print(' Maximum:', int(np.max(title_len)) )
print('\n', '-'*15, '\n')
print('"Clean Article body length"')
print(' Minimun:', int(np.min(body_len)) )
print(' Mode:', int(statistics.mode(body_len)) )
print(' 99 percentile:', int(np.percentile(body_len, 99)) )
print(' Maximum:', int(np.max(body_len)) )

"""### Doc2vec model

For both title and news body, word to vector model will be applied to convert them into vectors.
"""

# To combine two data and prepare it for the model
data = [ *list(df.tokenized_headline) , *list(df.tokenized_body) ]
tagged_data= [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]
print('Data is ready to create doc2vec model')

# To Train two doc2vec models for data in headlines and article body (this might takes some times)
d2v_head = Doc2Vec(vector_size=20, min_count=1, epochs=10)
d2v_head.build_vocab(tagged_data)
d2v_head.train(tagged_data, total_examples=d2v_head.corpus_count, epochs=d2v_head.epochs)

d2v = Doc2Vec(vector_size=150, min_count=1, epochs=10)
d2v.build_vocab(tagged_data)
d2v.train(tagged_data, total_examples=d2v.corpus_count, epochs=d2v.epochs)

print('Two Doc2Vec models for headlines and article bodies trained')

# Create vectors for titles and article body (this might takes some times)
df['headline_vec'] = df.tokenized_headline.apply(lambda x: d2v_head.infer_vector(x) )
df['body_vec'] = df.tokenized_body.apply(lambda x: d2v.infer_vector(x) )

print('Both titles and article body converted to document to vector format')

# To check the dataset
df = df[['clean_headline','clean_body','tokenized_headline','tokenized_body','headline_vec','body_vec','category']]
df.head()

"""### Polarity, Subjectivity, Refutation and Overlapping

Other four characteristics of headline and body that will be used in classification.  
The overlap function and the list of refuting words has been extracted from the baseline model.
"""

# To get the polarity and subjectivity of headlines and bodies by using textblob (this might takes some times)
df['headline_polarity'] = df.clean_headline.apply(lambda x: [TextBlob(x).sentiment.polarity])
df['body_polarity'] = df.clean_body.apply(lambda x: [TextBlob(x).sentiment.polarity])
df['headline_subjectivity'] = df.clean_headline.apply(lambda x: [TextBlob(x).sentiment.subjectivity])
df['body_subjectivity'] = df.clean_body.apply(lambda x: [TextBlob(x).sentiment.subjectivity])
print('Columns for polarity and subjectivity for both headline and article body added to the dataftame')

# To get the refution score from headlines and article bodies(this might takes some times)
refuting_words = ['fake','fraud','hoax','false','deny','denies','refute','not','despite','nope','doubt', 'doubts','bogus','debunk','pranks','retract']
df['headline_refutation_score'] = df.clean_headline.apply(lambda x: [refutation_score(x)])
df['body_refutation_score'] = df.clean_body.apply(lambda x: [refutation_score(x)])
print('Columns for refutation score of headline and article body added to the dataftame')

# To get the amount of overlapping between each headline and article body(this might takes some times)
df['overlapping_score'] = df.apply(lambda x: overlapping(x.tokenized_headline , x.tokenized_body), axis = 1)
print('Column for overlapping scores added to the dataftame')

"""### Finalizing vectors

Now is time to combine all feature vectors and make it ready for the neural model.
"""

# To create one vector for each pair of headline and body articles
df['vector']= df.apply(lambda x: [*x.headline_vec, *x.headline_polarity, *x.headline_subjectivity, *x.headline_refutation_score, *x.overlapping_score, *x.body_vec, *x.body_polarity, *x.body_subjectivity, *x.body_refutation_score ], axis = 1)
print('vector column created')

# To check the dataset
df = df[['clean_headline','clean_body','tokenized_headline','tokenized_body','headline_vec','body_vec','headline_polarity','body_polarity','headline_subjectivity','body_subjectivity','headline_refutation_score','body_refutation_score','overlapping_score','vector','category']]
df.head()

# To check vector column
vector_len = list(df.vector.apply(lambda x : len(x)))
print('The final vectors size is:', set(vector_len))

"""## Data pre_processing"""

# Splitting training data frame into two parts
x= np.array(list(df.vector))
y= to_categorical(np.array(df.category))

train_x , val_x ,train_y , val_y = train_test_split( x , y , test_size = 0.2)
print('Data has been split into training and validation parts')

# To create desirable format for the test dataframe(this will takes some times)
print('Preparing headline of test dataset')
test_df['tokenized_headline'] = test_df['Headline'].apply(lambda x: clean_text(x))
test_df['clean_headline'] = test_df['Headline'].apply(lambda x: no_char(x))
test_df['headline_vec'] = test_df.tokenized_headline.apply(lambda x: d2v_head.infer_vector(x))
test_df['headline_polarity'] = test_df.clean_headline.apply(lambda x: [TextBlob(x).sentiment.polarity])
test_df['headline_subjectivity'] = test_df.clean_headline.apply(lambda x: [TextBlob(x).sentiment.subjectivity])
test_df['headline_refutation_score'] = test_df.clean_headline.apply(lambda x: [refutation_score(x)])

print('Preparing article body of test dataset')
test_df['tokenized_body'] = test_df['articleBody'].apply(lambda x: clean_text(x))
test_df['clean_body'] = test_df['articleBody'].apply(lambda x: no_char(x))
test_df['body_vec'] = test_df.tokenized_body.apply(lambda x: d2v.infer_vector(x) )
test_df['body_polarity'] = test_df.clean_body.apply(lambda x: [TextBlob(x).sentiment.polarity])
test_df['body_subjectivity'] = test_df.clean_body.apply(lambda x: [TextBlob(x).sentiment.subjectivity])
test_df['body_refutation_score'] = test_df.clean_body.apply(lambda x: [refutation_score(x)])

print('Creating combination of headline and body vectors')
test_df['overlapping_score'] = test_df.apply(lambda x: overlapping(x.tokenized_headline , x.tokenized_body), axis = 1)
test_df['vector']= test_df.apply(lambda x: [*x.headline_vec, *x.headline_polarity, *x.headline_subjectivity, *x.headline_refutation_score, *x.overlapping_score, *x.body_vec, *x.body_polarity, *x.body_subjectivity, *x.body_refutation_score ], axis = 1)

print('Creating test_x from test dataset')
test_x = np.array(list(test_df.vector))

print('Test dataset is ready to use as "test_x"')

# To check the test dataset
test_df = test_df[['clean_headline','clean_body','tokenized_headline','tokenized_body','headline_vec','body_vec','headline_polarity','body_polarity','headline_subjectivity','body_subjectivity','headline_refutation_score','body_refutation_score','overlapping_score','vector']]
test_df.head()

# To check vector column of test set
test_vector_len = list(test_df.vector.apply(lambda x : len(x)))
print('The final test_set vectors size is:',set(test_vector_len))

"""## Classification models

Now, it is time to use the vector and create two models, a multominal logistic regression and a sequential neural networks, for stance detection.

### Multinominal logistic regression
"""

# To create a logistic regression classification model
lr_model = LogisticRegression(class_weight= 'balanced' , multi_class='multinomial')
lr_model.fit(train_x , np.argmax(train_y,axis=1) )

"""#### To check the model performance in validation set"""

# To apply model on validation set
lr_pred= lr_model.predict(val_x)
lr_report = classification_report(np.argmax(val_y,axis=1), lr_pred, target_names = ['agree','disagree','discuss','unrelated'] )
print(lr_report)

# To graph the confusion matrix
lr_cm=confusion_matrix(np.argmax(val_y,axis=1), lr_pred)
lr_cm = pd.DataFrame(lr_cm)
lr_cm.index.name = 'Actual'
lr_cm.columns.name = 'Predicted'

fig, axis= plt.subplots(figsize=(10, 8))
plt.tight_layout()

ax_label = ['agree','disagree','discuss','unrelated']
sns.heatmap(lr_cm ,cmap= "Blues",annot = True, fmt='', xticklabels=ax_label, yticklabels=ax_label)
sns.set(font_scale=1.2)

fig.savefig('Images/lr_confusion_matrix.png')
plt.show()

"""#### Creating .csv data to be evaluate in codaLab"""

lr_test_pred = lr_model.predict(test_x)
result= [cat_dic[int(x)] for x in lr_test_pred]  
print(' Count of each class in result is as below:', '\n', { x : result.count(x) for x in set(result) })

# give a csv file with answers(this code will overright existing folder!)
answers = test_stances_unlabeled.copy()
answers['Stance'] = result
answers.to_csv('data/answer.csv', index=False, encoding='utf-8')
print('"answers.csv" saved in data folder of the repository')

"""### Neural network model"""

# Defining Sequentional model 
model = Sequential(name='Neural_Model')
model.add(Dense(256, activation= 'relu', input_dim=177 ))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(128,activation='relu'))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(32,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(4, activation='sigmoid'))

model.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-6, decay= 0.01, momentum=0.99 , nesterov=True), loss='categorical_crossentropy',metrics=['accuracy'])

model.summary()

# To plot the Neural Model
plot_model(model, to_file='Images/model_plot.png', show_shapes=True, show_layer_names=False)

# To compute weights for the model based on the training set weights
train_y_int= [y.argmax() for y in train_y]
compute_weights = class_weight.compute_class_weight('balanced',np.unique(train_y_int),train_y_int)
weights = dict(enumerate(compute_weights))
# Cat: {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}
print(' The computed weights of the model will be set as:', '\n' , weights)

# To set weights and fit the model (this takes some times)
history = model.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), class_weight=weights)
print('\n','*'*20,'Model trained','*'*20)

# To save sequentioal model
model.save('data/sequential_model')
print('Sequential model has been saved')

# Plotting the accuracy and loss of the training and validation sets during epochs
fig, (ax1,ax2)= plt.subplots(ncols=2, figsize=(10, 5), dpi=100)

epochs = [i for i in range(50)]
train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
fig.set_size_inches(20,10)

ax1.plot(epochs , train_acc , 'co-' , label = 'Training Accuracy')
ax1.plot(epochs , val_acc , 'yo-' , label = 'Validation Accuracy')
ax1.set_title('Training & Validation accuracy per epochs',fontsize=12, fontweight='bold')
ax1.legend()
ax1.set_xlabel("Epochs")
ax1.set_ylabel("accuracy")
ax1.set_facecolor('white')
ax1.spines['bottom'].set_color('0.5')
ax1.spines['top'].set_color('0.5')
ax1.spines['right'].set_color('0.5')
ax1.spines['left'].set_color('0.5')

ax2.plot(epochs , train_loss , 'co-' , label = 'Training Loss')
ax2.plot(epochs , val_loss , 'yo-' , label = 'Validation Loss')
ax2.set_title('Training & Validation Loss per epochs', fontsize=12, fontweight='bold')
ax2.legend( fontsize= 13)
ax2.set_xlabel("Epochs")
ax2.set_ylabel("Loss")
ax2.set_facecolor('white')
ax2.spines['bottom'].set_color('0.5')
ax2.spines['top'].set_color('0.5')
ax2.spines['right'].set_color('0.5')
ax2.spines['left'].set_color('0.5')

fig.suptitle('Fitting model history per epoches',fontweight='bold')

fig.savefig('Images/learning_loss.png')
plt.show()

"""#### Checking model performace in validation set"""

# To apply model on validation set
pred_prop =model.predict(val_x)
pred_class=np.around(pred_prop , decimals = 0)
report = classification_report(val_y, pred_class , target_names = ['agree','disagree','discuss','unrelated'] )
print(report)

# To graph the confusion matrix of valodation set
cm=confusion_matrix(np.argmax(val_y, axis=1),np.argmax(pred_class,axis=1))
cm = pd.DataFrame(cm)
cm.index.name = 'Actual'
cm.columns.name = 'Predicted'

fig, axis= plt.subplots(figsize=(10, 8))
plt.tight_layout()

ax_label = ['agree','disagree','discuss','unrelated']
sns.heatmap(cm ,cmap= "Blues",annot = True, fmt='', xticklabels=ax_label, yticklabels=ax_label )
sns.set(font_scale=1.2)

fig.savefig('Images/confusion_matrix.png')
plt.show()

"""#### Creating .csv data to be evaluate in codaLab

To evaluate the model, a .csv file in a special format is needed.
"""

# To apply model on test set and get results based on the "cat_dic" dictionary
pred_prop =model.predict(test_x)
pred_class = np.argmax(pred_prop, axis=-1)
result= [cat_dic[int(x)] for x in pred_class]  
print(' Count of each class in result is as below:', '\n', { x : result.count(x) for x in set(result) })

# give a csv file with answers(this code will overright existing folder!)
answers = test_stances_unlabeled.copy()
answers['Stance'] = result
answers.to_csv('data/answer.csv', index=False, encoding='utf-8')
print('"answers.csv" saved in data folder of the repository')